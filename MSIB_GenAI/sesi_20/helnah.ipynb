{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=api_key)\n",
    "\n",
    "\n",
    "def generate_text(prompt, model=\"meta-llama/Llama-3.2-3B-Instruct\", max_tokens=500):\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": prompt\n",
    "\t\t}\n",
    "\t]\n",
    "\n",
    "\tstream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=True\n",
    "\t)\n",
    "\n",
    "\tresponse_text = \"\"\n",
    "\tfor chunk in stream:\n",
    "\t\tresponse_text += chunk.choices[0].delta.content\n",
    "\treturn response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "A: The capital of France is Paris.\n",
      "\n",
      "Q: What is the capital of Japan?\n",
      "A: The capital of Japan is Tokyo.\n",
      "\n",
      "Q: What is the capital of Australia?\n",
      "A: The capital of Australia is Canberra.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "for country in countries:\n",
    "    question = f\"What is the capital of {country}?\"\n",
    "    response = generate_text(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of Sweden?\n",
      "A: The capital of Sweden is Stockholm.\n",
      "\n",
      "Q: What is the capital of Mexico?\n",
      "A: The capital of Mexico is Mexico City (also known as Ciudad de MÃ©xico). It is the largest city in Mexico and the country's political, economic, and cultural center.\n",
      "\n",
      "Q: What is the capital of Vietnam?\n",
      "A: The capital of Vietnam is Hanoi.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"What is the capital of {country}?\"\n",
    ")\n",
    "\n",
    "def chain_invoke_api(prompt, input_var):\n",
    "    formatted_prompt = prompt.format(**input_var)\n",
    "    return generate_text(formatted_prompt)\n",
    "\n",
    "\n",
    "countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "for country in countries:\n",
    "    response = chain_invoke_api(prompt, {\"country\": country})\n",
    "    print(f\"Q: {prompt.format(country=country)}\")\n",
    "    print(f\"A: {response}\\n\")\n",
    "\n",
    "# Example 3: Sequential Chains using API\n",
    "pt1 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a random question about {topic}: Question: \"\n",
    ")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Local\\Temp\\ipykernel_16868\\3791850439.py:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pdf = './pdf/what-is-generative-ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    ").from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=generate_text,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Machine Learning?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {response['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
