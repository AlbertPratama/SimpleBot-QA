{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load API_KEY from env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('HUGGING_FACE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "# client = InferenceClient(api_key=api_key)\n",
    "\n",
    "# def generate_text(prompt, model=\"meta-llama/Llama-3.2-3B-Instruct\", max_tokens=500):\n",
    "# \tmessages = [\n",
    "# \t\t{\n",
    "# \t\t\t\"role\": \"user\",\n",
    "# \t\t\t\"content\": prompt\n",
    "# \t\t}\n",
    "# \t]\n",
    "\n",
    "# \tstream = client.chat.completions.create(\n",
    "# \t\t\tmodel=model,\n",
    "# \t\t\tmessages=messages,\n",
    "# \t\t\tmax_tokens=max_tokens,\n",
    "# \t\t\tstream=True\n",
    "# \t)\n",
    "\n",
    "# \tresponse_text = \"\"\n",
    "# \tfor chunk in stream:\n",
    "# \t\t\tresponse_text += chunk.choices[0].delta.content\n",
    "# \treturn response_text\n",
    "# \t\t\t# print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=api_key)\n",
    "\n",
    "\n",
    "def generate_text(prompt):\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": prompt\n",
    "\t\t}\n",
    "\t]\n",
    "\n",
    "\tstream = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=messages,\n",
    "            max_tokens=500,\n",
    "            stream=True\n",
    "\t)\n",
    "\n",
    "\tresponse_text = \"\"\n",
    "\tfor chunk in stream:\n",
    "\t\tresponse_text += chunk.choices[0].delta.content\n",
    "\treturn response_text\n",
    "\t# print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to your question would be:\\n\\nA: Dwight D. Eisenhower \\n Party: Republican'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_zhot_promt = \"\"\"\n",
    "                  Q: Who was President of the United States in 1955? Which party did he belong to?\\n\n",
    "                  A:\n",
    "                  \"\"\"\n",
    "\n",
    "resp = generate_text(zero_zhot_promt)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several ways to answer this question, as it depends on the criteria used to define \"the tallest mountain in the world.\" Here are a few different answers:\\n\\n1. Height above sea level: The tallest mountain in the world above sea level is Mount Everest, located in the Himalayas on the border between Nepal and Tibet, China. It stands at a height of 8,848.86 meters (29,031.7 feet) above sea level.\\n\\n2. Highest point on Earth: If we consider the highest point on Earth regardless of whether it\\'s above sea level or not, then it\\'s Mauna Kea, a dormant volcano on the Big Island of Hawaii. Measured from its base on the ocean floor, Mauna Kea is over 10,200 meters (33,465 feet) tall.\\n\\n3. Tallest mountain when measured from base to peak: This title belongs to Mauna Mauna, which is an underwater volcano in the Pacific Ocean. Measured from its base on the ocean floor, Mauna Mauna is over 9,700 meters (31,800 feet) tall, making it the tallest mountain in the world when measured from base to peak.\\n\\nIt\\'s worth noting that the definition of \"tallest mountain\" can be subjective and depend on the context in which it\\'s being used.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_zhot_promt = \"\"\"\n",
    "                  Q: What is the tallest mountain in the world?\\n\n",
    "                  A:\n",
    "                  \"\"\"\n",
    "resp = generate_text(zero_zhot_promt)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arthur Fleming or originally isolated  but for the mass production of the Gel or powder penicillin it was Howard Florey'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Q: Who is the current President of France?\\n\n",
    "            A: Emmanuel Macron \\n\\n\n",
    "\n",
    "            Q: Who invented the telephone? \\n\n",
    "            A: Alexander Graham Bell \\n\\n\n",
    "\n",
    "            Q: Who wrote the novel \"1984\"?\n",
    "            A: George Orwell \\n\\n\n",
    "\n",
    "            Q: Who discovered penicillin?\n",
    "            A:\n",
    "        \"\"\"\n",
    "\n",
    "resp = generate_text(prompt)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "# headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "\n",
    "# def query(payload):\n",
    "#     response = requests.post(API_URL, headers=headers, json=payload)\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# prompt = \"\"\" Please answer only the last question below.\n",
    "# Q: Who is the current President of France?\\n\n",
    "# A: Emmanuel Macron \\n\\n\n",
    "# Q: Who invented the telephone? \\n\n",
    "# A: Alexander Graham Bell \\n\\n\n",
    "# Q: Who wrote the novel \"1984\"?\n",
    "# A: George Orwell \\n\\n\n",
    "# Q: Who discovered penicillin?\n",
    "# A:\n",
    "#         \"\"\"\n",
    "\n",
    "# payload = {\n",
    "#     \"inputs\": prompt,\n",
    "#     \"parameters\": {\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"top_k\": 40,\n",
    "#         \"max_length\": 20,  # Batasi output lebih ketat\n",
    "#         \"stop\": [\"\\n\\n\", \"Q:\"]\n",
    "#     },\n",
    "#     \"options\": {\n",
    "#         \"use_cache\": False\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# generate_res = query(payload)\n",
    "# print(generate_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = {\n",
    "#     \"inputs\": prompt,\n",
    "#     \"options\": {\n",
    "#         \"use_cache\": False\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Kirim permintaan ke API dan ambil hasilnya\n",
    "# generate_res = query(payload)\n",
    "\n",
    "# # Cetak hasil output\n",
    "# if 'generated_text' in generate_res:\n",
    "#     print(generate_res['generated_text'].strip())\n",
    "# else:\n",
    "#     print(\"Error:\", generate_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # URL untuk model facebook/bart-large-mnli\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
    "# # Ganti dengan API key Anda\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\"\n",
    "# }\n",
    "\n",
    "\n",
    "# def query(payload):\n",
    "#     response = requests.post(API_URL, headers=headers, json=payload)\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# # Definisikan prompt dan candidate labels\n",
    "# prompt = \"Who discovered penicillin?\"\n",
    "# candidate_labels = [\"Alexander Fleming\",\n",
    "#                     \"Louis Pasteur\", \"Marie Curie\", \"Thomas Edison\"]\n",
    "\n",
    "# # Siapkan payload untuk API\n",
    "# payload = {\n",
    "#     \"inputs\": prompt,\n",
    "#     \"parameters\": {\n",
    "#         \"candidate_labels\": candidate_labels\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Kirim permintaan ke API dan ambil hasilnya\n",
    "# generate_res = query(payload)\n",
    "\n",
    "# # Cetak hasil output\n",
    "# if 'labels' in generate_res:\n",
    "#     print(\"Predicted Label:\", generate_res['labels'][0])\n",
    "# else:\n",
    "#     print(\"Error:\", generate_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# outputs_1 = query({\n",
    "# \t\"inputs\": \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "# })\n",
    "\n",
    "# print(outputs_1)\n",
    "# print()\n",
    "# for i, j in enumerate(outputs_1):\n",
    "#   for res in j:\n",
    "#     print(f\"{res['label']} : {res['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import traceback\n",
    "# from time import sleep\n",
    "\n",
    "# try:\n",
    "#     from langchain import PromptTemplate\n",
    "#     from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
    "#     from langchain.document_loaders import PyPDFLoader\n",
    "#     from langchain.indexes import VectorstoreIndexCreator\n",
    "#     from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#     from langchain.text_splitter import CharacterTextSplitter\n",
    "#     from langchain.llms import HuggingFacePipeline\n",
    "#     from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "# except ImportError as e:\n",
    "#     print(traceback.format_exc())\n",
    "#     print(e)\n",
    "\n",
    "# # Initialize the model and tokenizer\n",
    "# model_id = \"google/flan-ul2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# # Create the pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=512,\n",
    "#     temperature=0.2,\n",
    "#     top_p=1,\n",
    "#     top_k=25,\n",
    "#     repetition_penalty=1.0\n",
    "# )\n",
    "\n",
    "# # Create LangChain HF pipeline\n",
    "# local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # Example 1: Basic Question-Answering\n",
    "# countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "# for country in countries:\n",
    "#     question = f\"What is the capital of {country}?\"\n",
    "#     response = local_llm(question)\n",
    "#     print(f\"Q: {question}\")\n",
    "#     print(f\"A: {response}\\n\")\n",
    "\n",
    "# # Example 2: Using Prompt Templates\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"country\"],\n",
    "#     template=\"What is the capital of {country}?\"\n",
    "# )\n",
    "\n",
    "# chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "\n",
    "# countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "# for country in countries:\n",
    "#     response = chain.invoke({\"country\": country})\n",
    "#     print(f\"Q: {prompt.format(country=country)}\")\n",
    "#     print(f\"A: {response['text']}\\n\")\n",
    "\n",
    "# # Example 3: Sequential Chains\n",
    "# pt1 = PromptTemplate(\n",
    "#     input_variables=[\"topic\"],\n",
    "#     template=\"Generate a random question about {topic}: Question: \"\n",
    "# )\n",
    "# pt2 = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"Answer the following question: {question}\"\n",
    "# )\n",
    "\n",
    "# prompt_to_model_1 = LLMChain(llm=local_llm, prompt=pt1)\n",
    "# prompt_to_model_2 = LLMChain(llm=local_llm, prompt=pt2)\n",
    "# qa = SimpleSequentialChain(\n",
    "#     chains=[prompt_to_model_1, prompt_to_model_2],\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Example topic\n",
    "# qa.invoke(\"an animal\")\n",
    "\n",
    "# # Example 4: RAG with PDF\n",
    "# pdf = 'what-is-generative-ai.pdf'\n",
    "# loaders = [PyPDFLoader(pdf)]\n",
    "\n",
    "# index = VectorstoreIndexCreator(\n",
    "#     embedding=HuggingFaceEmbeddings(),\n",
    "#     text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# ).from_loaders(loaders)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=local_llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=index.vectorstore.as_retriever(),\n",
    "#     input_key=\"question\"\n",
    "# )\n",
    "\n",
    "# # Test RAG\n",
    "# question = \"what is Machine Learning?\"\n",
    "# response = chain.invoke({\"question\": question})\n",
    "# print(f\"Q: {question}\")\n",
    "# print(f\"A: {response['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "Answer the question given in the contex below:\n",
      "Context: \n",
      "Storage and content policy \n",
      "\n",
      "How durable is my data in Cloud Storage? \n",
      "\n",
      "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
      "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
      "across multiple devices located in multiple availability zones.\n",
      "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
      "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
      "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
      "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
      "?\n",
      "\n",
      "Question: How is high availability achieved? \n",
      "\n",
      "Answer:\n",
      "\n",
      "[Response]\n",
      "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the write is acknowledged as successful, ensuring high availability.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Storage and content policy \\n\n",
    "How durable is my data in Cloud Storage? \\n\n",
    "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
    "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
    "across multiple devices located in multiple availability zones.\n",
    "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
    "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
    "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
    "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How is high availability achieved?\"\n",
    "\n",
    "prompt = f\"\"\"Answer the question given in the contex below:\n",
    "Context: {context}?\\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "gen_res = generate_text(prompt=prompt)\n",
    "print(gen_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction-Tuning output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "Answer the question given the context below as {Context:}. \n",
      "\n",
      "If the answer is not available in the {Context:} and you are not confident about the output,\n",
      "please say \"Information not available in provided context\". \n",
      "\n",
      "\n",
      "Context: \n",
      "Storage and content policy \n",
      "\n",
      "How durable is my data in Cloud Storage? \n",
      "\n",
      "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
      "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
      "across multiple devices located in multiple availability zones.\n",
      "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
      "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
      "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
      "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
      "?\n",
      "\n",
      "Question: What machined are required for hosting Vertex AI models? \n",
      "\n",
      "Answer:\n",
      "\n",
      "[Response]\n",
      "Information not available in provided context.\n"
     ]
    }
   ],
   "source": [
    "question = \"What machined are required for hosting Vertex AI models?\"\n",
    "prompt = f\"\"\"Answer the question given the context below as {{Context:}}. \\n\n",
    "If the answer is not available in the {{Context:}} and you are not confident about the output,\n",
    "please say \"Information not available in provided context\". \\n\\n\n",
    "Context: {context}?\\n\n",
    "Question: {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"[Prompt]\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"[Response]\")\n",
    "resp = generate_text(prompt=prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Mona Lisa was painted by Leonardo da Vinci. \\n\\nJohn McCarthy discovered the term \"artificial intelligence\".'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "The term \"artificial intelligence\" was first coined by John McCarthy in 1956. Since then, AI has developed into a vast\n",
    "field with numerous applications, ranging from self-driving cars to virtual assistants like Siri and Alexa.\n",
    "\n",
    "Question:\n",
    "What is artificial intelligence?\n",
    "\n",
    "Answer:\n",
    "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "The Wright brothers, Orville and Wilbur, were two American aviation pioneers who are credited with inventing and\n",
    "building the world's first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight,\n",
    "on December 17, 1903.\n",
    "\n",
    "Question:\n",
    "Who were the Wright brothers?\n",
    "\n",
    "Answer:\n",
    "The Wright brothers were American aviation pioneers who invented and built the world's first successful airplane\n",
    "and made the first controlled, powered and sustained heavier-than-air human flight, on December 17, 1903.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "The Mona Lisa is a 16th-century portrait painted by Leonardo da Vinci during the Italian Renaissance. It is one of\n",
    "the most famous paintings in the world, known for the enigmatic smile of the woman depicted in the painting.\n",
    "\n",
    "Question:\n",
    "Who painted the Mona Lisa?\n",
    "Who discovered AI term?\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = generate_text(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = []\n",
    "# for answer in response.split('\\n'):\n",
    "#     stripped_answer = answer.strip()\n",
    "#     if stripped_answer:  # Memastikan tidak menambahkan jawaban kosong\n",
    "#         answers.append(stripped_answer)\n",
    "# # Menformat output\n",
    "# formatted_output = \"\"\n",
    "# for answer in answers:\n",
    "#     formatted_output += f\"'{answer}'\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Mona Lisa was painted by Leonardo da Vinci.', 'John McCarthy discovered the term \"artificial intelligence\".']\n",
      "'The Mona Lisa was painted by Leonardo da Vinci.'\n",
      "'John McCarthy discovered the term \"artificial intelligence\".'\n"
     ]
    }
   ],
   "source": [
    "ans = [answer.strip() for answer in response.split('\\n') if answer.strip()]\n",
    "ans_for = \"\\n\".join([f\"'{answer}'\" for answer in ans])\n",
    "print(ans)\n",
    "print(ans_for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lower rainfall in the Amazon during the LGM was almost certainly associated with reduced moist tropical vegetation cover in the basin.\\n\\nIt's worth noting that the information is a bit incomplete in the question, as it asks for attribution without the full preceding sentence. According to the provided text, the reduced moist tropical vegetation cover is what led to lower rainfall in the Amazon during the LGM, rather than the other way around.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Background: There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation.\n",
    "Analyses of sediment deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly\n",
    "associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small,\n",
    "isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today.\n",
    "This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both\n",
    "explanations are reasonably well supported by the available data.\n",
    "\n",
    "Q: What does LGM stands for?\n",
    "A: Last Glacial Maximum.\n",
    "\n",
    "Q: What did the analysis from the sediment deposits indicate?\n",
    "A: Rainfall in the basin during the LGM was lower than for the present.\n",
    "\n",
    "Q: What are some of scientists arguments?\n",
    "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
    "\n",
    "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
    "A: 21,000.\n",
    "\n",
    "Q: What caused changes in the Amazon rainforest vegetation?\n",
    "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
    "\n",
    "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
    "A: Sediment deposits.\n",
    "\n",
    "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "res = generate_text(prompt)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “w...</td>\n",
       "      <td>World Wide Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer_groundtruth\n",
       "0  In a website browser address bar, what does “w...     World Wide Web\n",
       "1       Who was the first woman to win a Nobel Prize        Marie Curie\n",
       "2     What is the name of the Earth’s largest ocean?  The Pacific Ocean"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data = {\n",
    "    \"question\": [\n",
    "        \"In a website browser address bar, what does “www” stand for?\",\n",
    "        \"Who was the first woman to win a Nobel Prize\",\n",
    "        \"What is the name of the Earth’s largest ocean?\",\n",
    "    ],\n",
    "    \"answer_groundtruth\": [\"World Wide Web\", \"Marie Curie\", \"The Pacific Ocean\"],\n",
    "}\n",
    "qa_data_df = pd.DataFrame(qa_data)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "      <th>answer_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “w...</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>\"WWW\" stands for \"World Wide Web.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>Marie Curie was the first woman to win a Nobel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>The Pacific Ocean.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer_groundtruth  \\\n",
       "0  In a website browser address bar, what does “w...     World Wide Web   \n",
       "1       Who was the first woman to win a Nobel Prize        Marie Curie   \n",
       "2     What is the name of the Earth’s largest ocean?  The Pacific Ocean   \n",
       "\n",
       "                                   answer_prediction  \n",
       "0                 \"WWW\" stands for \"World Wide Web.\"  \n",
       "1  Marie Curie was the first woman to win a Nobel...  \n",
       "2                                 The Pacific Ocean.  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answer(row):\n",
    "    prompt = f\"\"\"Answer the following question as precise as possible.\\n\\n\n",
    "            question: {row}\n",
    "            answer:\n",
    "              \"\"\"\n",
    "    return generate_text(prompt=prompt)\n",
    "\n",
    "\n",
    "qa_data_df[\"answer_prediction\"] = qa_data_df[\"question\"].apply(get_answer)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_groundtruth</th>\n",
       "      <th>answer_prediction</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a website browser address bar, what does “w...</td>\n",
       "      <td>World Wide Web</td>\n",
       "      <td>\"WWW\" stands for \"World Wide Web.\"</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first woman to win a Nobel Prize</td>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>Marie Curie was the first woman to win a Nobel...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the Earth’s largest ocean?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>The Pacific Ocean.</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer_groundtruth  \\\n",
       "0  In a website browser address bar, what does “w...     World Wide Web   \n",
       "1       Who was the first woman to win a Nobel Prize        Marie Curie   \n",
       "2     What is the name of the Earth’s largest ocean?  The Pacific Ocean   \n",
       "\n",
       "                                   answer_prediction  match_score  \n",
       "0                 \"WWW\" stands for \"World Wide Web.\"          100  \n",
       "1  Marie Curie was the first woman to win a Nobel...          100  \n",
       "2                                 The Pacific Ocean.          100  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def get_fuzzy_match(df):\n",
    "    return fuzz.partial_ratio(df[\"answer_groundtruth\"], df[\"answer_prediction\"])\n",
    "\n",
    "\n",
    "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average match score of all predicted answer from Llama-3.2-3B-Instruct is :  100.0  %\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"the average match score of all predicted answer from Llama-3.2-3B-Instruct is : \",\n",
    "    qa_data_df[\"match_score\"].mean(),\n",
    "    \" %\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
