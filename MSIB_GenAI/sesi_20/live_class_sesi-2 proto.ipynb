{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "A: london\n",
      "\n",
      "Q: What is the capital of Japan?\n",
      "A: nagasaki\n",
      "\n",
      "Q: What is the capital of Australia?\n",
      "A: melbourne\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Inference API details\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def api_based_llm(input_text):\n",
    "    response = query({\"inputs\": input_text})\n",
    "    return response[0][\"generated_text\"] if isinstance(response, list) else response\n",
    "\n",
    "\n",
    "# Example 1: Basic Question-Answering using API\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "for country in countries:\n",
    "    question = f\"What is the capital of {country}?\"\n",
    "    response = api_based_llm(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\\n\")\n",
    "\n",
    "# Example 2: Using Prompt Templates\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"What is the capital of {country}?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of Sweden?\n",
      "A: sweden\n",
      "\n",
      "Q: What is the capital of Mexico?\n",
      "A: san juan\n",
      "\n",
      "Q: What is the capital of Vietnam?\n",
      "A: vietnam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def chain_invoke_api(prompt, input_var):\n",
    "    formatted_prompt = prompt.format(**input_var)\n",
    "    return api_based_llm(formatted_prompt)\n",
    "\n",
    "\n",
    "countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "for country in countries:\n",
    "    response = chain_invoke_api(prompt, {\"country\": country})\n",
    "    print(f\"Q: {prompt.format(country=country)}\")\n",
    "    print(f\"A: {response}\\n\")\n",
    "\n",
    "# Example 3: Sequential Chains using API\n",
    "pt1 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a random question about {topic}: Question: \"\n",
    ")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the most common way to kill a fox?\n",
      "Answer: slicing it with a sharp knife\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Local\\Temp\\ipykernel_20592\\4246269487.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "C:\\Users\\Albert\\AppData\\Local\\Temp\\ipykernel_20592\\4246269487.py:16: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sequential_chain_api(pt1, pt2, topic):\n",
    "    generated_question = api_based_llm(pt1.format(topic=topic))\n",
    "    answer = api_based_llm(pt2.format(question=generated_question))\n",
    "    print(f\"Generated Question: {generated_question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "\n",
    "# Example topic\n",
    "sequential_chain_api(pt1, pt2, \"an animal\")\n",
    "\n",
    "# Example 4: RAG with PDF (Preserved for context, requires adaptation to API calls)\n",
    "pdf = './pdf/what is generative ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    ").from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chain = RetrievalQA.from_chain_type(\n",
    "# #     llm=api_based_llm,\n",
    "# #     chain_type=\"stuff\",\n",
    "# #     retriever=index.vectorstore.as_retriever(),\n",
    "# #     input_key=\"question\"\n",
    "# # )\n",
    "\n",
    "# # Test RAG (This will depend on adapting the Retriever to work seamlessly with API calls)\n",
    "# question = \"What is Machine Learning?\"\n",
    "# response = chain.invoke({\"question\": question})\n",
    "# print(f\"Q: {question}\")\n",
    "# print(f\"A: {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\AppData\\Local\\Temp\\ipykernel_20592\\3867811952.py:24: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding=HuggingFaceEmbeddings(),\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is standfor GPT?\n",
      "A: generative pretrained transformer\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Define the API endpoint and headers for Hugging Face Inference API\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "# Function to query the language model API\n",
    "\n",
    "\n",
    "def query_api(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    response_data = response.json()\n",
    "    return response_data[0].get(\"generated_text\", \"Error in response\") if isinstance(response_data, list) else str(response_data)\n",
    "\n",
    "\n",
    "# Step 1: Load and index documents (RAG Retrieval)\n",
    "pdf = './pdf/what-is-generative-ai.pdf'  # Replace with your PDF file\n",
    "loaders = [PyPDFLoader(pdf)]\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    ").from_loaders(loaders)\n",
    "\n",
    "# Step 2: Retrieve relevant context for a given query\n",
    "\n",
    "\n",
    "def retrieve_context(query):\n",
    "    retriever = index.vectorstore.as_retriever()\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context\n",
    "\n",
    "# Step 3: Perform RAG using serverless API-based language model\n",
    "\n",
    "\n",
    "def perform_rag(query):\n",
    "    # Retrieve context for the query\n",
    "    context = retrieve_context(query)\n",
    "    # Construct the prompt for the API call\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    # Query the language model API for the answer\n",
    "    response = query_api(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example query\n",
    "question = \"What is standfor GPT?\"\n",
    "response = perform_rag(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is standfor AI?\n",
      "A: Generative artificial intelligence (AI) describes algorithms (such as ChatGPT) that can\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "question = \"What is standfor AI?\"\n",
    "response = perform_rag(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize the model and tokenizer\n",
    "# model_id = \"google/flan-ul2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# # Create the pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=512,\n",
    "#     temperature=0.2,\n",
    "#     top_p=1,\n",
    "#     top_k=25,\n",
    "#     repetition_penalty=1.0\n",
    "# )\n",
    "\n",
    "# # Create LangChain HF pipeline\n",
    "# local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # Example 1: Basic Question-Answering\n",
    "# countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "# for country in countries:\n",
    "#     question = f\"What is the capital of {country}?\"\n",
    "#     response = local_llm(question)\n",
    "#     print(f\"Q: {question}\")\n",
    "#     print(f\"A: {response}\\n\")\n",
    "\n",
    "# # Example 2: Using Prompt Templates\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"country\"],\n",
    "#     template=\"What is the capital of {country}?\"\n",
    "# )\n",
    "\n",
    "# chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "\n",
    "# countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "# for country in countries:\n",
    "#     response = chain.invoke({\"country\": country})\n",
    "#     print(f\"Q: {prompt.format(country=country)}\")\n",
    "#     print(f\"A: {response['text']}\\n\")\n",
    "\n",
    "# # Example 3: Sequential Chains\n",
    "# pt1 = PromptTemplate(\n",
    "#     input_variables=[\"topic\"],\n",
    "#     template=\"Generate a random question about {topic}: Question: \"\n",
    "# )\n",
    "# pt2 = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"Answer the following question: {question}\"\n",
    "# )\n",
    "\n",
    "# prompt_to_model_1 = LLMChain(llm=local_llm, prompt=pt1)\n",
    "# prompt_to_model_2 = LLMChain(llm=local_llm, prompt=pt2)\n",
    "# qa = SimpleSequentialChain(\n",
    "#     chains=[prompt_to_model_1, prompt_to_model_2],\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Example topic\n",
    "# qa.invoke(\"an animal\")\n",
    "\n",
    "# # Example 4: RAG with PDF\n",
    "# pdf = 'what-is-generative-ai.pdf'\n",
    "# loaders = [PyPDFLoader(pdf)]\n",
    "\n",
    "# index = VectorstoreIndexCreator(\n",
    "#     embedding=HuggingFaceEmbeddings(),\n",
    "#     text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# ).from_loaders(loaders)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=local_llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=index.vectorstore.as_retriever(),\n",
    "#     input_key=\"question\"\n",
    "# )\n",
    "\n",
    "# # Test RAG\n",
    "# question = \"what is Machine Learning?\"\n",
    "# response = chain.invoke({\"question\": question})\n",
    "# print(f\"Q: {question}\")\n",
    "# print(f\"A: {response['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
